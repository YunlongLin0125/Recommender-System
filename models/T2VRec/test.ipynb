{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from Time2Vec.periodic_activations import SineActivation, CosineActivation\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "l1 = SineActivation(1, 50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "a = torch.randint(1, 10, (2, 2, 1), dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class Time2vec(nn.Module):\n",
    "    def __init__(self, c_in, c_out, activation=\"cos\"):\n",
    "        super().__init__()\n",
    "        self.wnbn = nn.Linear(c_in, c_out - 1, bias=True)\n",
    "        self.w0b0 = nn.Linear(c_in, 1, bias=True)\n",
    "        self.act = torch.cos if activation == \"cos\" else torch.sin\n",
    "\n",
    "    def forward(self, x):\n",
    "        part0 = self.act(self.w0b0(x))\n",
    "        # print(part0.shape)\n",
    "        part1 = self.act(self.wnbn(x))\n",
    "        # print(part1.shape)\n",
    "        return torch.cat([part0, part1], -1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "l1 = Time2vec(1, 50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[5., 5., 8., 6., 4., 8., 7., 1., 3., 9., 8., 2., 4., 7., 1., 1., 9.,\n          1., 3., 1., 3., 5., 8., 7., 9., 1., 7., 8., 5., 8., 3., 1., 4., 4.,\n          4., 7., 1., 9., 7., 3., 5., 4., 8., 2., 9., 8., 9., 9., 9., 5.]],\n\n        [[6., 2., 1., 5., 2., 8., 3., 8., 9., 4., 2., 7., 7., 5., 1., 8., 4.,\n          9., 6., 4., 7., 6., 6., 7., 7., 6., 2., 9., 4., 8., 1., 9., 7., 5.,\n          7., 2., 2., 8., 9., 2., 9., 6., 8., 5., 2., 3., 2., 7., 7., 6.]]])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x50 and 1x1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43ml1\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[21], line 9\u001B[0m, in \u001B[0;36mTime2vec.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m----> 9\u001B[0m     part0 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mact(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mw0b0\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;66;03m# print(part0.shape)\u001B[39;00m\n\u001B[0;32m     11\u001B[0m     part1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mact(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwnbn(x))\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (2x50 and 1x1)"
     ]
    }
   ],
   "source": [
    "l1(a)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"int\") to list",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[32], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m x \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m3\u001B[39m]\n\u001B[1;32m----> 2\u001B[0m \u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\n",
      "\u001B[1;31mTypeError\u001B[0m: can only concatenate list (not \"int\") to list"
     ]
    }
   ],
   "source": [
    "x = [1,2,3]\n",
    "x + 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 2])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2],\n",
    "                  [2, 1]])\n",
    "a.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 2, 2])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b =  torch.tensor([[[1, 2], [2, 1]],\n",
    "                   [[1, 2], [2, 1]]])\n",
    "b.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 1, 2])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.unsqueeze(1).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([2, 2, 2]),\n tensor([[[2, 4],\n          [4, 2]],\n \n         [[2, 4],\n          [4, 2]]]))"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a + b).shape, a+b"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[[1, 6, 3, 8],\n           [1, 9, 4, 1],\n           [0, 3, 3, 0]],\n \n          [[9, 6, 1, 2],\n           [2, 8, 7, 3],\n           [1, 6, 7, 4]]],\n \n \n         [[[8, 1, 0, 6],\n           [5, 2, 4, 7],\n           [9, 6, 5, 4]],\n \n          [[8, 6, 1, 4],\n           [9, 2, 4, 0],\n           [1, 0, 3, 0]]]], dtype=torch.int32),\n tensor([[[9, 6, 1, 2],\n          [2, 8, 7, 3],\n          [1, 6, 7, 4]],\n \n         [[8, 6, 1, 4],\n          [9, 2, 4, 0],\n          [1, 0, 3, 0]]], dtype=torch.int32),\n torch.Size([2, 3, 4]))"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "# Set the random seed for reproducibility (optional)\n",
    "random.seed(42)\n",
    "\n",
    "# Create a torch.tensor of size (3, 4) with random values between 0 and 1\n",
    "a = torch.randint(0, 10, (2, 2, 3, 4), dtype=torch.int32)\n",
    "b = a[:, -1, :]\n",
    "a, b, b.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "a = torch.randint(0, 10, (2, 1), dtype=torch.int32)\n",
    "b = torch.randint(0, 10, (2, 2), dtype=torch.int32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[6],\n         [8]], dtype=torch.int32),\n tensor([[0, 9],\n         [8, 2]], dtype=torch.int32))"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 6, 15],\n        [16, 10]], dtype=torch.int32)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[9, 7, 1, 5],\n         [7, 3, 7, 0]], dtype=torch.int32),\n tensor([[[4, 3, 1, 6],\n          [6, 9, 7, 1],\n          [2, 2, 4, 8]],\n \n         [[7, 9, 1, 7],\n          [2, 8, 0, 2],\n          [4, 6, 4, 8]]], dtype=torch.int32))"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randint(0, 10, (2, 4), dtype=torch.int32)\n",
    "b = torch.randint(0, 10, (2, 3, 4), dtype=torch.int32)\n",
    "a, b"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "a = torch.randint(0,1, (1, 1, 10), dtype=torch.int32)\n",
    "periods = torch.tensor([0.25, 0.5, 0.75, 1, 2, 4, 8, 16, 24, 7*24, 28*24, 365*24]*3600, dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (43200) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[34], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43ma\u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43mperiods\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: The size of tensor a (10) must match the size of tensor b (43200) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "a/periods"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[9, 7, 1, 5]],\n\n        [[7, 3, 7, 0]]], dtype=torch.int32)"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.unsqueeze(-2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "pos_logits = (a.unsqueeze(-2) * b)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[36, 21,  1, 30],\n         [54, 63,  7,  5],\n         [18, 14,  4, 40]],\n\n        [[49, 27,  7,  0],\n         [14, 24,  0,  0],\n         [28, 18, 28,  0]]], dtype=torch.int32)"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_logits"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class Time2Vec_abs(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Time2Vec_abs, self).__init__()\n",
    "        self.P = 12\n",
    "        self.periods = torch.tensor([0.25*3600, 0.5*3600, 0.75*3600, 1*3600, 2*3600, 4*3600, 8*3600, 16*3600, 24*3600\n",
    "                                        , 7*24*3600, 28*24*3600, 365*24*3600], dtype=torch.float32)\n",
    "        self.phase_sin = torch.nn.Parameter(torch.randn(self.P), requires_grad=True)\n",
    "        self.phase_cos = torch.nn.Parameter(torch.randn(self.P), requires_grad=True)\n",
    "\n",
    "    def forward(self, time_seq):\n",
    "        # relative time features\n",
    "        batch_size= time_seq.shape[0]\n",
    "        seq_len = time_seq.shape[1]\n",
    "        time_seq = time_seq.unsqueeze(-1)\n",
    "        periods_exp = self.periods.unsqueeze(0).unsqueeze(0)\n",
    "        time_scaled = time_seq / periods_exp  # divide by the periods\n",
    "        features_cos = torch.cos((2* np.pi * time_scaled) + self.phase_cos)\n",
    "        features_sin = torch.sin((2* np.pi * time_scaled) + self.phase_sin)\n",
    "        features_log = torch.log(time_seq)\n",
    "\n",
    "        stacked_features = torch.stack((features_cos, features_sin), dim=-1)\n",
    "        stacked_features = stacked_features.view(batch_size, seq_len,-1)\n",
    "        features_log = features_log\n",
    "        concat_features = torch.cat([stacked_features, features_log], dim=-1)\n",
    "        return concat_features\n",
    "\n",
    "\n",
    "t2v = Time2Vec_abs()\n",
    "a = t2v(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "a.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "data = torch.randint(0, 1000000, (2, 3, 3), dtype=torch.int32)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[358095, 620121, 290337],\n         [ 86281, 661793,  70281],\n         [678833, 153314, 619700]],\n\n        [[207788, 696571, 503367],\n         [747926, 907336, 741619],\n         [940001,  13087, 195393]]], dtype=torch.int32)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[620121, 661793, 153314],\n        [696571, 907336,  13087]], dtype=torch.int32)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[..., 1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([2, 3, 12]),\n torch.Size([2, 3, 12]),\n torch.Size([2, 3, 24]),\n torch.Size([2, 3, 1]),\n torch.Size([2, 3, 25]))"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape, b.shape, c.shape, d.shape, e.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([ 0.8586,  0.4725,  0.7051,  0.6866,  0.7811, -0.1613,  0.1674, -0.3789,\n         -0.3511,  0.9992,  0.7286,  0.9522], grad_fn=<SelectBackward0>),\n tensor([-0.2799,  0.4996, -0.2968, -0.8747, -0.8488,  0.3596,  0.1649,  0.8939,\n         -0.6596, -0.4216, -0.4306,  0.9997], grad_fn=<SelectBackward0>),\n tensor([ 0.8586, -0.2799,  0.4725,  0.4996,  0.7051, -0.2968,  0.6866, -0.8747,\n          0.7811, -0.8488, -0.1613,  0.3596,  0.1674,  0.1649, -0.3789,  0.8939,\n         -0.3511, -0.6596,  0.9992, -0.4216,  0.7286, -0.4306,  0.9522,  0.9997],\n        grad_fn=<SelectBackward0>),\n tensor([2.0794]),\n tensor([ 0.8586, -0.2799,  0.4725,  0.4996,  0.7051, -0.2968,  0.6866, -0.8747,\n          0.7811, -0.8488, -0.1613,  0.3596,  0.1674,  0.1649, -0.3789,  0.8939,\n         -0.3511, -0.6596,  0.9992, -0.4216,  0.7286, -0.4306,  0.9522,  0.9997,\n          2.0794], grad_fn=<SelectBackward0>))"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0,0], b[0,0], c[0,0], d[0,0], e[0,0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 32])\n",
      "torch.Size([2, 3, 32])\n",
      "torch.Size([2, 3, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class Time2Vec_rel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Time2Vec_rel, self).__init__()\n",
    "        self.P = 32\n",
    "        self.periods = torch.logspace(np.log10(1), np.log10(4*7*24*3600), self.P)\n",
    "        self.phase_sin = torch.nn.Parameter(torch.randn(self.P), requires_grad=True)\n",
    "        self.phase_cos = torch.nn.Parameter(torch.randn(self.P), requires_grad=True)\n",
    "\n",
    "    def forward(self, time_seq):\n",
    "        # relative time features\n",
    "        batch_size= time_seq.shape[0]\n",
    "        seq_len = time_seq.shape[1]\n",
    "        time_seq = time_seq.unsqueeze(-1)\n",
    "        periods_exp = self.periods.unsqueeze(0).unsqueeze(0)\n",
    "        time_scaled = time_seq / periods_exp  # divide by the periods\n",
    "        features_cos = torch.cos((2* np.pi * time_scaled) + self.phase_cos)\n",
    "        features_sin = torch.sin((2* np.pi * time_scaled) + self.phase_sin)\n",
    "        features_log = torch.log(time_seq)\n",
    "        stacked_features = torch.stack((features_cos, features_sin), dim=-1)\n",
    "        stacked_features = stacked_features.view(batch_size, seq_len,-1)\n",
    "        features_log = features_log\n",
    "        concat_features = torch.cat([stacked_features, features_log], dim=-1)\n",
    "        return concat_features\n",
    "\n",
    "t2v_rel = Time2Vec_rel()\n",
    "a, b, c, d, e = t2v_rel(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([ 0.9812,  0.7213,  0.9280,  0.8491, -0.5070, -0.2947, -0.9098,  0.5542,\n          0.4496,  0.9972,  0.2903,  0.5482,  0.7048,  0.8395,  0.7438,  0.9027,\n          0.8852,  0.5587,  0.9885,  0.9711,  0.4583,  0.0995,  0.3147,  0.5652,\n          0.6809,  0.9035,  0.4413,  0.9594,  0.6966,  0.9997,  0.9883,  0.9958],\n        grad_fn=<SelectBackward0>),\n tensor([-0.1743, -0.9322,  0.3337,  0.6507,  0.3062, -0.6344,  0.8506,  0.5733,\n          0.9571,  0.4531,  0.9584,  0.8163, -0.3037, -0.9887,  0.3409,  0.6202,\n         -0.9908, -0.9766,  0.6305,  0.3558,  0.2376, -0.0075, -0.9236, -0.4971,\n          1.0000, -0.7613, -0.9252,  0.7560, -0.5194, -0.9500, -0.6805,  0.6033],\n        grad_fn=<SelectBackward0>),\n tensor([ 0.9812, -0.1743,  0.7213, -0.9322,  0.9280,  0.3337,  0.8491,  0.6507,\n         -0.5070,  0.3062, -0.2947, -0.6344, -0.9098,  0.8506,  0.5542,  0.5733,\n          0.4496,  0.9571,  0.9972,  0.4531,  0.2903,  0.9584,  0.5482,  0.8163,\n          0.7048, -0.3037,  0.8395, -0.9887,  0.7438,  0.3409,  0.9027,  0.6202,\n          0.8852, -0.9908,  0.5587, -0.9766,  0.9885,  0.6305,  0.9711,  0.3558,\n          0.4583,  0.2376,  0.0995, -0.0075,  0.3147, -0.9236,  0.5652, -0.4971,\n          0.6809,  1.0000,  0.9035, -0.7613,  0.4413, -0.9252,  0.9594,  0.7560,\n          0.6966, -0.5194,  0.9997, -0.9500,  0.9883, -0.6805,  0.9958,  0.6033],\n        grad_fn=<SelectBackward0>),\n tensor([2.0794]),\n tensor([ 0.9812, -0.1743,  0.7213, -0.9322,  0.9280,  0.3337,  0.8491,  0.6507,\n         -0.5070,  0.3062, -0.2947, -0.6344, -0.9098,  0.8506,  0.5542,  0.5733,\n          0.4496,  0.9571,  0.9972,  0.4531,  0.2903,  0.9584,  0.5482,  0.8163,\n          0.7048, -0.3037,  0.8395, -0.9887,  0.7438,  0.3409,  0.9027,  0.6202,\n          0.8852, -0.9908,  0.5587, -0.9766,  0.9885,  0.6305,  0.9711,  0.3558,\n          0.4583,  0.2376,  0.0995, -0.0075,  0.3147, -0.9236,  0.5652, -0.4971,\n          0.6809,  1.0000,  0.9035, -0.7613,  0.4413, -0.9252,  0.9594,  0.7560,\n          0.6966, -0.5194,  0.9997, -0.9500,  0.9883, -0.6805,  0.9958,  0.6033,\n          2.0794], grad_fn=<SelectBackward0>))"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0,0], b[0,0], c[0,0], d[0,0], e[0,0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([2, 3, 32]),\n torch.Size([2, 3, 32]),\n torch.Size([2, 3, 64]),\n torch.Size([2, 3, 1]),\n torch.Size([2, 3, 65]))"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape, b.shape, c.shape, d.shape, e.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([9.0000e+02, 1.8000e+03, 2.7000e+03, 3.6000e+03, 7.2000e+03, 1.4400e+04,\n        2.8800e+04, 5.7600e+04, 8.6400e+04, 6.0480e+05, 2.4192e+06, 3.1536e+07])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([0.25, 0.5, 0.75, 1, 2, 4, 8, 16, 24, 7*24, 28*24, 365*24]) * 3600"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "class Time2Vec_abs(torch.nn.Module):\n",
    "    def __init__(self, periods):\n",
    "        super(Time2Vec_abs, self).__init__()\n",
    "        self.P = periods\n",
    "        self.periods = torch.tensor(\n",
    "            [0.25 * 3600, 0.5 * 3600, 0.75 * 3600, 1 * 3600, 2 * 3600, 4 * 3600, 8 * 3600, 16 * 3600, 24 * 3600\n",
    "                , 7 * 24 * 3600, 28 * 24 * 3600, 365 * 24 * 3600], dtype=torch.float32)\n",
    "        self.phase_sin = torch.nn.Parameter(torch.randn(self.P), requires_grad=True)\n",
    "        self.phase_cos = torch.nn.Parameter(torch.randn(self.P), requires_grad=True)\n",
    "\n",
    "    def forward(self, time_seq):\n",
    "        # relative time features\n",
    "        batch_size = time_seq.shape[0]\n",
    "        seq_len = time_seq.shape[1]\n",
    "        time_seq = time_seq.unsqueeze(-1)\n",
    "        periods_exp = self.periods.unsqueeze(0).unsqueeze(0)\n",
    "        periods_exp = periods_exp\n",
    "        time_scaled = time_seq / periods_exp  # divide by the periods\n",
    "        features_cos = torch.cos((2 * np.pi * time_scaled) + self.phase_cos)\n",
    "        features_sin = torch.sin((2 * np.pi * time_scaled) + self.phase_sin)\n",
    "        features_log = torch.log(time_seq)\n",
    "        stacked_features = torch.stack((features_cos, features_sin), dim=-1)\n",
    "        stacked_features = stacked_features.view(batch_size, seq_len, -1)\n",
    "        features_log = features_log\n",
    "        concat_features = torch.cat([stacked_features, features_log], dim=-1)\n",
    "        return concat_features\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "t2v_abs = Time2Vec_abs(12)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "r = t2v_abs(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[324124, 625816, 501223],\n        [ 29305, 374396, 161464]], dtype=torch.int32)"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 8.1646e-01,  3.7902e-01,  4.6314e-01, -3.1135e-01, -2.9784e-01,\n           6.3701e-01,  6.3793e-01, -4.3623e-01,  4.4739e-01,  3.1015e-01,\n          -7.8072e-01,  4.0006e-01,  3.4092e-01,  8.2771e-01, -9.9757e-01,\n          -8.2925e-01, -1.3332e-01, -5.7070e-01, -8.7598e-01, -3.2508e-01,\n          -2.0294e-01,  6.0557e-01,  9.5560e-01, -7.8371e-01,  1.2689e+01],\n         [-3.7551e-01,  9.8748e-01,  1.8769e-01, -3.4628e-01, -9.2892e-01,\n          -8.1749e-01,  9.3707e-01, -9.9325e-01, -1.5336e-01, -2.9793e-01,\n          -5.5378e-01,  6.5977e-01, -4.8149e-01, -9.0419e-01, -7.5599e-03,\n          -6.2120e-01,  1.8415e-01,  6.1221e-01,  8.7950e-01,  3.1812e-01,\n          -8.3488e-01,  9.9066e-01,  9.3617e-01, -7.4498e-01,  1.3347e+01],\n         [ 7.0532e-01, -8.4934e-01, -9.2585e-01,  8.5124e-01, -2.7345e-01,\n          -9.5511e-01, -5.0391e-01,  6.9508e-01, -8.7902e-01, -7.9881e-01,\n          -3.6162e-01, -9.9432e-01, -5.5635e-01,  3.7117e-02, -8.5842e-01,\n          -9.9231e-01,  1.7796e-01, -2.9048e-01,  6.9785e-01, -8.2525e-01,\n          -6.1653e-01,  8.9590e-01,  9.4461e-01, -7.6131e-01,  1.3125e+01]],\n\n        [[-9.9108e-01,  9.2839e-02, -7.5020e-01,  8.4860e-01, -9.9817e-01,\n          -4.9443e-01,  2.6858e-02,  2.1191e-01,  7.1479e-01,  6.0345e-01,\n           8.7336e-01, -2.4290e-01,  9.6515e-01,  6.2794e-01, -7.8122e-01,\n          -2.3179e-01, -4.0555e-01,  5.6028e-02,  8.3531e-01,  3.9848e-01,\n           5.3233e-01, -1.1498e-01,  9.7125e-01, -8.1882e-01,  1.0286e+01],\n         [ 9.6154e-01, -4.8384e-01,  8.0067e-01, -6.9146e-01, -4.3219e-01,\n          -9.9145e-01,  7.9272e-01, -6.2478e-01,  3.4489e-01,  2.0222e-01,\n           7.4461e-01, -4.5061e-01,  9.3020e-01,  5.3782e-01, -7.4538e-01,\n          -1.7745e-01, -3.7145e-01,  9.2937e-02, -5.1858e-01, -7.5350e-01,\n          -3.2870e-01,  7.0402e-01,  9.5260e-01, -7.7745e-01,  1.2833e+01],\n         [-6.5947e-01,  8.8066e-01,  3.4873e-01, -4.9791e-01, -9.6443e-01,\n          -7.4815e-01,  9.6298e-01, -9.9947e-01,  1.1189e-01,  2.5768e-01,\n           8.2088e-01,  7.6512e-01, -9.5723e-01, -9.4487e-01, -3.8483e-01,\n          -8.7125e-01,  5.6768e-01,  1.2967e-01, -3.7489e-01,  9.7760e-01,\n           2.1638e-01,  2.2604e-01,  9.6465e-01, -8.0342e-01,  1.1992e+01]]],\n       grad_fn=<CatBackward0>)"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TimeEncoder_abs(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TimeEncoder_abs, self).__init__()\n",
    "        # Set the periods for absolute and relative time\n",
    "        self.P_abs = 12\n",
    "        # The vector phi is a learned parameter\n",
    "        self.phi = nn.Parameter(torch.Tensor(2 * self.P_abs))\n",
    "        # Manually chosen periods of real-life importance\n",
    "        self.abs_periods = (torch.tensor([0.25, 0.5, 0.75, 1, 2, 4, 8, 16, 24, 168, 672, 8760],\n",
    "                                         dtype=torch.float32) * 3600)\n",
    "        # Log scale periods for relative time difference\n",
    "\n",
    "    def forward(self, t):\n",
    "        # Calculate the features for absolute and relative time\n",
    "        r_abs = self.encode_time(t, self.P_abs, self.abs_periods)\n",
    "        return r_abs\n",
    "\n",
    "    def encode_time(self, t, P, periods):\n",
    "        t = t.float()\n",
    "        periods = periods.unsqueeze(0).unsqueeze(0)\n",
    "        features = []\n",
    "        for i in range(1, P + 1):\n",
    "            pi = periods[..., i - 1]\n",
    "            phi_2i_minus_1 = self.phi[2 * i - 2]\n",
    "            phi_2i = self.phi[2 * i - 1]\n",
    "            features.append(torch.cos(2 * np.pi * t / pi + phi_2i_minus_1))\n",
    "            features.append(torch.sin(2 * np.pi * t / pi + phi_2i))\n",
    "\n",
    "        features.append(torch.log(t))\n",
    "        return torch.cat(features, dim=-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "time_seqs = torch.randint(0, 1000000, (128, 200, 2), dtype=torch.int32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([128, 200, 2])"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_seqs.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([128, 200])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_seqs[..., 0].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for dimension 1 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[40], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtime_seqs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mshape\n",
      "\u001B[1;31mIndexError\u001B[0m: index 2 is out of bounds for dimension 1 with size 2"
     ]
    }
   ],
   "source": [
    "time_seqs[..., 1].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([128, 200, 1])"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_seqs.unsqueeze(-1).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "t2v = TimeEncoder_abs()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "t_emb = t2v(time_seqs.unsqueeze(-1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 8.6956e-01, -4.9383e-01, -9.6684e-01,  ...,  9.9820e-01,\n          -5.9184e-01,  1.2616e+01],\n         [ 9.7352e-01,  2.2858e-01, -9.9336e-01,  ...,  9.8412e-01,\n          -5.9184e-01,  1.3705e+01],\n         [ 3.0229e-01,  9.5322e-01,  8.0694e-01,  ...,  9.9956e-01,\n          -5.9184e-01,  1.1916e+01],\n         ...,\n         [-9.9961e-01,  2.7928e-02, -1.3965e-02,  ...,  9.9989e-01,\n          -5.9184e-01,  1.1227e+01],\n         [ 6.4283e-01, -7.6601e-01,  9.0632e-01,  ...,  9.9904e-01,\n          -5.9184e-01,  1.2299e+01],\n         [ 4.0026e-01, -9.1640e-01, -8.3674e-01,  ...,  9.9696e-01,\n          -5.9184e-01,  1.2877e+01]],\n\n        [[ 4.2601e-01, -9.0472e-01,  8.4440e-01,  ...,  9.9787e-01,\n          -5.9184e-01,  1.2699e+01],\n         [ 7.4777e-01,  6.6396e-01,  9.3482e-01,  ...,  9.8434e-01,\n          -5.9184e-01,  1.3698e+01],\n         [-2.7960e-02, -9.9961e-01,  6.9715e-01,  ...,  9.8547e-01,\n          -5.9184e-01,  1.3661e+01],\n         ...,\n         [-4.9380e-01, -8.6957e-01,  5.0309e-01,  ...,  9.9859e-01,\n          -5.9184e-01,  1.2492e+01],\n         [-8.1310e-01,  5.8212e-01, -3.0569e-01,  ...,  1.0000e+00,\n          -5.9184e-01,  8.0265e+00],\n         [-3.0938e-01,  9.5094e-01, -5.8763e-01,  ...,  9.8056e-01,\n          -5.9184e-01,  1.3807e+01]],\n\n        [[-8.9563e-01, -4.4480e-01,  2.2844e-01,  ...,  9.8984e-01,\n          -5.9184e-01,  1.3481e+01],\n         [ 1.0000e+00, -2.7324e-04, -1.0000e+00,  ...,  9.8732e-01,\n          -5.9184e-01,  1.3592e+01],\n         [ 2.7541e-01,  9.6133e-01,  7.9856e-01,  ...,  9.8642e-01,\n          -5.9184e-01,  1.3627e+01],\n         ...,\n         [-2.0813e-01,  9.7810e-01, -6.2923e-01,  ...,  9.9522e-01,\n          -5.9184e-01,  1.3104e+01],\n         [ 2.0856e-02,  9.9978e-01, -7.1444e-01,  ...,  9.9846e-01,\n          -5.9184e-01,  1.2537e+01],\n         [ 9.7661e-01,  2.1501e-01,  9.9414e-01,  ...,  9.9478e-01,\n          -5.9184e-01,  1.3148e+01]],\n\n        ...,\n\n        [[ 4.1925e-02, -9.9912e-01,  7.2178e-01,  ...,  9.9829e-01,\n          -5.9184e-01,  1.2589e+01],\n         [ 9.4209e-01,  3.3536e-01,  9.8542e-01,  ...,  9.9222e-01,\n          -5.9184e-01,  1.3348e+01],\n         [-9.2190e-01,  3.8743e-01,  1.9761e-01,  ...,  9.9531e-01,\n          -5.9184e-01,  1.3095e+01],\n         ...,\n         [ 6.5359e-01, -7.5685e-01,  9.0928e-01,  ...,  9.9801e-01,\n          -5.9184e-01,  1.2666e+01],\n         [-7.8360e-01, -6.2127e-01, -3.2894e-01,  ...,  9.8271e-01,\n          -5.9184e-01,  1.3748e+01],\n         [ 1.8035e-01,  9.8360e-01,  7.6823e-01,  ...,  9.8205e-01,\n          -5.9184e-01,  1.3767e+01]],\n\n        [[ 9.9881e-01, -4.8782e-02, -9.9970e-01,  ...,  9.9895e-01,\n          -5.9184e-01,  1.2344e+01],\n         [-7.8805e-01, -6.1561e-01, -3.2553e-01,  ...,  9.9558e-01,\n          -5.9184e-01,  1.3065e+01],\n         [-2.1232e-02,  9.9977e-01, -6.9956e-01,  ...,  9.9089e-01,\n          -5.9184e-01,  1.3427e+01],\n         ...,\n         [-8.7633e-01, -4.8170e-01, -2.4866e-01,  ...,  9.9859e-01,\n          -5.9184e-01,  1.2495e+01],\n         [ 4.8776e-01,  8.7298e-01, -8.6249e-01,  ...,  9.9277e-01,\n          -5.9184e-01,  1.3311e+01],\n         [ 8.5171e-01,  5.2402e-01,  9.6221e-01,  ...,  9.9964e-01,\n          -5.9184e-01,  1.1814e+01]],\n\n        [[ 3.7479e-01, -9.2711e-01,  8.2909e-01,  ...,  9.9660e-01,\n          -5.9184e-01,  1.2933e+01],\n         [-6.1015e-01, -7.9229e-01,  4.4150e-01,  ...,  9.9981e-01,\n          -5.9184e-01,  1.1500e+01],\n         [ 9.9979e-01, -2.0705e-02, -9.9995e-01,  ...,  9.8113e-01,\n          -5.9184e-01,  1.3792e+01],\n         ...,\n         [-2.7530e-01, -9.6136e-01,  6.0196e-01,  ...,  9.8999e-01,\n          -5.9184e-01,  1.3474e+01],\n         [ 3.4910e-02,  9.9939e-01,  7.1934e-01,  ...,  9.9995e-01,\n          -5.9184e-01,  1.0832e+01],\n         [-9.1068e-01,  4.1311e-01,  2.1133e-01,  ...,  9.9753e-01,\n          -5.9184e-01,  1.2775e+01]]], grad_fn=<CatBackward0>)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_emb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([128, 200, 25])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_emb.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "l1 = [(1,2), (2,3)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "[(1, 10), (2, 1)]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip([1,2], [10,1]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def time_feature_creation_rel(User):\n",
    "    # relative time feature\n",
    "    print(\"Time feature deriving: Rel\")\n",
    "    for user in User:\n",
    "        items = list(map(lambda x: x[0], User[user]))\n",
    "        timestamps = list(map(lambda x: x[1], User[user]))\n",
    "        time_gaps = [timestamps[i+1] - timestamps[i] for i in range(len(timestamps) - 1)]\n",
    "        time_gaps.append(0)\n",
    "        User[user] = list(zip(items, time_gaps))\n",
    "    print(\"feature Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "User = dict()\n",
    "User[0] = [(1,1), [2,11], [3, 21]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time feature deriving: Rel\n",
      "feature Done\n"
     ]
    }
   ],
   "source": [
    "time_feature_creation_rel(User)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: [(1, 10), (2, 10), (3, 0)]}"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "User"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "x = torch.logspace(np.log10(1), np.log10(4 * 7 * 24 * 3600), 32).float()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(2419200.)"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[-1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(31536000.)"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(8760 * 3600).float()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "[(0, (1, 2)), (1, (3, 4))]"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip([0,1], [(1,2), [3,4]]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "tuple"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = (1,2,3,4)\n",
    "type(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
