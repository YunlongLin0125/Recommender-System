So this week I focus on the split strategy and pinnerformer.
I will first introduce 4 types of split methods I learned from a paper.


first one item-based approach or you can say interaction-based. this is the commonly used method for next-item prediction, I think I mention this in week 1 when introducing SASRec.
we treat each user as a single data, in this method the final interaction per user is for testing, and the second last transaction is for validation and the remaining transactions can be used for training. I think this method cannot be used in our task, because we are not next-item prediction.

in user-based split method, we just split the data by different users, split all users to training set, validation set and test set. This method is not commonly used. Because in this method, it requires that the model can recommend items for new(cold-start) users, this is very challenging in many approachs.

Random split, as its name, it just randomly selects the boundary per user. this method has been abandoned, because it is reproducible.

the last method is Temporal split, two kinds of temporal split the first one teporal user split, this method is similar to leave one last approach, the difference is here we not only need one for testing but we choose a percentage of last interaction for testing, and another percentage of last interaction for validation and the remaining is training set.

in the temporal global split defines fixed time-point for all users, and the interation after that point are used for testing. The temporal split can also be used in our task.

And I also propose two method of how to split the dataset in our task, in the first one I use temporal split, here we split each user to training, validation and test,
here is the example of how to implement it in the one week time window, I am not quite whether its correct.
Here we just split a part of actions to training data, and choose the development sets and training sets required by the time window. Then when testing, we combine all datas before this point as the input and find the other output fit the time window to evaluate the model performance. Is this correct?
And I also want to know we should choose a certain percentage of data as training data or we should pre-define a fixed-point, which one is better?

The other method I mentioned here is based on user-based split method, which is much easier to implement.


I also do some reasearch on the pinnerformer, which my teammates mentioned in the last week, The model is designed to capture the complex interactions between users and Pins(items), The main idea of this model learns user embeddings from user's action sequence. And rather than only focus on the next-item prediction the model also has a similar window architecture, and they want to predict all actions in the window. And they also provide a new loss function, dense all action loss, in this loss function, the model will not only focus on the final user embedding or you can say the most recent user embedding. They made each user embedding during the calculation has some effects on the prediction result rather than only focus on the most recent user embedding, this loss function help the model to learn.
And for the split strategy, I think they are using user-based split method to split the dataset, but they did not mentioned explicitly in the paper. why they choose user-based split method.


Question:
Different models for different time windows, because we need to prepare for different training data because of this different time windows. 
Or we just train a general model, which a parameter to control the time window. I think hard for this, different models will be more reasonable.

Which split methods should we choose for our task.


Why they choose user based 